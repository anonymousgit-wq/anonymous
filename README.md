RL-TPO++ extends Test-Time Preference Optimization (TPO) [Li et al., ICML 2025] into a multi-agent framework running on 8GB laptops. Four agents (Policy, Reward, Critique, Refinement) iteratively align LLMs at inference using FsfairX-LLaMA3-RM-v0.1 and Llama-3.1-Tulu-3-8B-RM, with optional RL distillation.

Achieves SOTA safety on WildGuard, XSTest, BeaverTails at 0.01% compute vs. RLHF.

üöÄ Quick Start (Local Ollama)
Prerequisites
Python 3.10+

Ollama (local LLMs): Install Ollama

NVIDIA GPU ‚â•8GB or Apple Silicon

bash
# Clone repo
git clone "https://github.com/anonymousgit-wq/anonymous"
cd rl-tpo-plus-plus

# Create environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
Pull Models
bash
ollama pull llama3.2:3b  # Policy/Refinement (lightweight)
ollama pull llama3.1:8b  # Alternative for stronger agents
ollama pull qwen2.5:7b-instruct  # Reward model proxy
Run Test-Time Optimization
bash
# Single prompt demo (RewardBench-style)
python run_rltpo.py \
  --prompt "Explain quantum entanglement safely" \
  --iterations 3 \
  --samples-per-iter 8 \
  --policy-model "llama3.2:3b" \
  --reward-model "qwen2.5:7b-instruct" \
  --output-dir ./outputs/demo

# Multi-prompt benchmark (WildGuard/XSTest)
python benchmarks/run_eval.py \
  --dataset wildguard \
  --model llama3.2:3b \
  --max-prompts 100
Output: Refined responses + trajectories in ./outputs/ for distillation.

üõ†Ô∏è Full Setup (Advanced)
1. Ollama Server (Recommended for Multi-Agent)
bash
# Start Ollama serve
ollama serve

# In new terminal, assign models to tags
ollama create policy-agent -m llama3.2:3b
ollama create reward-agent -m qwen2.5:7b-instruct
ollama create critique-agent -m llama3.1:8b
ollama create refinement-agent -m llama3.2:3b
2. Multi-Agent Pipeline
bash
# Full RL-TPO++ loop with distillation prep
python src/multi_agent_rltpo.py \
  --config configs/rewardbench.yaml \
  --trajectory-save ./trajectories/ \
  --distill  # Collect for PPO/GRPO
3. RL Distillation (Optional)
Requires ‚â•16GB VRAM or DeepSpeed.

bash
pip install torch transformers trl[ppo] deepspeed
deepspeed --num_gpus 1 train_distill.py \
  --trajectories ./trajectories/ \
  --base-model llama3.2:3b \
  --output rl-tpo-policy
üìä Benchmarks & Results
Benchmark	Base Llama-3.2-3B	RL-TPO++ (3 iters)	Improvement
WildGuard	72.1%	89.4%	+17.3% 
‚Äã
XSTest	65.3%	82.7%	+17.4% 
‚Äã
BeaverTails	78.2%	91.6%	+13.4% 
‚Äã
RewardBench	61.8%	79.2%	+17.4% 
‚Äã
Compute: 8GB GPU, <1min/prompt batch.

üèóÔ∏è Architecture
text
Prompt x ‚Üí Policy Agent (K samples) ‚Üí Reward Agent (scores) ‚Üí Critique Agent (feedback) ‚Üí Refinement Agent ‚Üí Loop T iters
                                                                 ‚Üì
                                                          Best y* + Trajectory œÑ for Distillation
Agents use Ollama API endpoints for zero-overhead switching.

üîß Configuration
Edit configs/default.yaml:

text
policy_model: "llama3.2:3b"
reward_model: "sfairXC/FsfairX-LLaMA3-RM-v0.1"  # HF/Ollama
iterations: 3
samples_per_iter: 8
max_tokens: 2048
Supported rewards: FsfairX, Tulu-RM, custom.

üì± Docker (One-Command)
bash
docker build -t rltpo .
docker run --gpus all -p 11434:11434 -v ./outputs:/app/outputs rltpo \
  --prompt "Your prompt here"
Dockerfile included.

üß™ Custom Datasets
Add JSONL:

json
{"prompt": "Is this safe?", "category": "safety"}
bash
python benchmarks/custom_eval.py --data your_data.jsonl
Dependencies
text
torch>=2.1.0
transformers>=4.45.0
ollama>=0.3.0
trl>=0.9.0  # For distillation
vllm>=0.6.0  # Optional fast inference
pyyaml
datasets  # HuggingFace
Full requirements.txt provided.