# RL-TPO++ Ollama Model Config (Full 5-model stack)
# Verified: all tags work with ollama list

models:
  # Role 1: POLICY (TPO responses + P_update revisions)
  policy:
    tag: "llama3.1:8b-instruct-q4_K_M"
    temperature: 0.7
    top_p: 0.95
    max_tokens: 1024
    role: "Generate initial responses and apply textual gradients"

  # Role 2: PRIMARY RM (TPO scoring: chosen/rejected pairs)
  rm_primary:
    tag: "llama3.1:8b-instruct-q5_K_M"
    temperature: 0.0
    top_p: 1.0
    max_tokens: 256
    role: "Score responses, select best/worst for textual loss"

  # Role 3: LOSS CRITIC (P_loss: compare chosen vs rejected)
  loss_critic:
    tag: "qwen2.5:7b-instruct-q4_K_M"
    temperature: 0.1
    top_p: 0.9
    max_tokens: 512
    role: "Textual loss: critique why chosen > rejected + improvements"

  # Role 4: GRADIENT GENERATOR (P_grad: instructions from loss)
  gradient_gen:
    tag: "llama3.1:8b-instruct-q4_0"
    temperature: 0.3
    top_p: 0.95
    max_tokens: 256
    role: "Compress critique into concise bullet-point instructions"

  # Role 5: CONSENSUS RM (RL phase: maj@3 for stable rewards)
  consensus_rm:
    tag: "llama3.2:3b-instruct-q8_0"
    temperature: 0.0
    top_p: 1.0
    max_tokens: 128
    role: "Fast/light majority vote for PPO/GRPO rewards"

# TPO defaults (overrides in tpo_config.yaml)
tpo_defaults:
  n_samples: 5
  n_steps: 2
  temperature: 0.7
